# Wiki Whatiz - RAG-Powered Wikipedia Q&A

A beautiful, modern web application for question-answering using Retrieval-Augmented Generation (RAG) with Wikipedia knowledge.

![Wiki Whatiz](https://img.shields.io/badge/RAG-Powered-blue) ![Node.js](https://img.shields.io/badge/Node.js-18+-green) ![Python](https://img.shields.io/badge/Python-3.11+-yellow)

## Features

- ğŸ” **Hybrid Retrieval**: Combines BM25 and vector search for optimal results
- ğŸ¯ **Re-ranking**: Uses cross-encoder models to refine search results
- ğŸ”„ **Iterative RAG**: Query refinement for better answer quality
- ğŸš€ **GPU Accelerated**: Full CUDA support for fast inference
- ğŸ¨ **Beautiful UI**: Modern, dark theme inspired by Cabinet of Wonders
- âš¡ **Real-time**: Fast responses with detailed timing breakdown

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â”€â”€â”€â”€â–¶â”‚  Node.js API    â”‚â”€â”€â”€â”€â–¶â”‚  Python RAG     â”‚
â”‚   (HTML/CSS/JS) â”‚     â”‚  (Express)      â”‚     â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚                       â”‚                       â”‚
                                â–¼                       â–¼                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   FAISS       â”‚       â”‚   BM25        â”‚       â”‚   Ollama      â”‚
                        â”‚   (Vectors)   â”‚       â”‚   (Keywords)  â”‚       â”‚   (LLM)       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- Ollama with `llama3.1:8b` model
- NVIDIA GPU with CUDA (optional, but recommended)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/wiki-whatiz.git
   cd wiki-whatiz
   ```

2. **Set up Python environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux/Mac
   
   pip install -r requirements.txt
   pip install fastapi uvicorn
   ```

3. **Install Node.js dependencies**
   ```bash
   cd web
   npm install
   cd ..
   ```

4. **Start Ollama**
   ```bash
   ollama pull llama3.1:8b
   ollama serve
   ```

### Running the Application

1. **Start the Python API backend** (Terminal 1)
   ```bash
   .venv\Scripts\activate
   uvicorn api:app --reload --port 8000
   ```

2. **Start the Node.js frontend** (Terminal 2)
   ```bash
   cd web
   npm start
   ```

3. **Open in browser**
   ```
   http://localhost:3000
   ```

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/status` | GET | Check backend status and GPU info |
| `/api/warmup` | POST | Pre-load models into memory |
| `/api/ask` | POST | Ask a question and get RAG answer |
| `/api/retrieve` | POST | Debug: retrieve documents only |

### Example Request

```bash
curl -X POST http://localhost:3000/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Why is the sky blue?",
    "use_iterative": true,
    "use_rerank": true,
    "rerank_k": 5
  }'
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `PORT` | 3000 | Node.js server port |
| `PYTHON_API_URL` | http://localhost:8000 | Python backend URL |
| `OLLAMA_MODEL` | llama3.1:8b | LLM model to use |
| `EMBED_MODEL` | sentence-transformers/all-MiniLM-L6-v2 | Embedding model |
| `RERANK_MODEL` | cross-encoder/ms-marco-MiniLM-L-6-v2 | Re-ranking model |

## Project Structure

```
bda-rag-wiki/
â”œâ”€â”€ api.py                 # FastAPI backend
â”œâ”€â”€ app.py                 # Original Streamlit app
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ retrieval.py      # BM25, FAISS, reranking
â”‚   â”œâ”€â”€ rag_answer.py     # RAG pipeline
â”‚   â”œâ”€â”€ llm.py            # Ollama integration
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/        # FAISS index and BM25 pickles
â””â”€â”€ web/
    â”œâ”€â”€ package.json
    â”œâ”€â”€ server.js         # Express server
    â””â”€â”€ public/
        â”œâ”€â”€ index.html    # Frontend HTML
        â”œâ”€â”€ styles.css    # CSS styles
        â””â”€â”€ app.js        # Frontend JavaScript
```

## Deployment

### GitHub Pages (Frontend Only)

The static frontend can be deployed to GitHub Pages:

```bash
# Build and deploy
cd web/public
git init
git add .
git commit -m "Deploy"
git push -f git@github.com:yourusername/wiki-whatiz.git main:gh-pages
```

### Full Stack Deployment

For full deployment, you'll need:
1. A server with GPU (for optimal performance)
2. Docker or direct deployment
3. Reverse proxy (nginx) to combine frontend and API

## License

MIT License

## Acknowledgments

- [Cabinet of Wonders](https://cabinetofwonders.app/) - Design inspiration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector similarity search
- [Sentence Transformers](https://www.sbert.net/) - Embeddings
- [Ollama](https://ollama.ai/) - Local LLM inference
